#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Local Mode: Flexible arXiv Paper Search Script
æœ¬åœ°æ¨¡å¼ï¼šçµæ´»çš„ arXiv è®ºæ–‡æœç´¢è„šæœ¬

è¿è¡Œåœ¨æœ¬åœ°ç”µè„‘ï¼ŒæŒ‰éœ€æ‰‹åŠ¨æœç´¢è®ºæ–‡ï¼ˆåŒºåˆ«äºäº‘ç«¯è‡ªåŠ¨é‡‡é›†æ¨¡å¼ï¼‰
Run on local machine for manual on-demand paper searching (distinct from cloud auto-collection)

ç”¨æ³• | Usage:
    # æœç´¢è‡ªåŠ¨é©¾é©¶ç›¸å…³è®ºæ–‡ï¼ˆé»˜è®¤20ç¯‡ï¼‰
    python run_manual_search.py --keywords "autonomous driving"

    # æ·±åº¦å­¦ä¹ å’Œè®¡ç®—æœºè§†è§‰
    python run_manual_search.py --keywords '"deep learning" AND "computer vision"'

    # æŒ‡å®šç»“æœæ•°é‡
    python run_manual_search.py --keywords "V2X communication" --max-results 50

    # åªæœç´¢å…ƒæ•°æ®ï¼Œä¸ä¸‹è½½ PDF
    python run_manual_search.py --keywords "reinforcement learning" --no-pdf
"""

import argparse
import asyncio
import io
import os
import sys
from datetime import datetime
from pathlib import Path
from typing import Optional

# Load environment variables from .env file
from dotenv import load_dotenv
ENV_FILE = Path(__file__).parent.parent / ".env"
load_dotenv(ENV_FILE)

from paperflow.utils.collection_logger import CollectionLogger

try:
    from rich.console import Console
    from rich.panel import Panel
    from rich.progress import BarColumn, Progress, SpinnerColumn, TaskProgressColumn, TextColumn
    from rich.table import Table

    RICH_AVAILABLE = True
except ImportError:
    RICH_AVAILABLE = False

# Fix Windows encoding issue
if sys.platform == "win32":
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding="utf-8")
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding="utf-8")

from paperflow import ArxivSearchParams, ArxivZoteroCollector
from paperflow.utils import ConfigLoader
from paperflow.utils.errors import ConfigError


def validate_keywords(keywords: str) -> str:
    """éªŒè¯æœç´¢å…³é”®è¯çš„åˆæ³•æ€§å’Œé•¿åº¦"""
    if not keywords or not keywords.strip():
        raise ValueError("å…³é”®è¯ä¸èƒ½ä¸ºç©º")

    keywords = keywords.strip()

    # é•¿åº¦éªŒè¯
    if len(keywords) > 500:
        raise ValueError(f"å…³é”®è¯è¿‡é•¿ï¼ˆæœ€å¤š500å­—ç¬¦ï¼Œå½“å‰: {len(keywords)}å­—ç¬¦ï¼‰")

    # æ£€æŸ¥æ½œåœ¨çš„æ³¨å…¥æ”»å‡»å­—ç¬¦
    dangerous_chars = [";", "\n", "\r", "\x00", "\x1a"]
    if any(char in keywords for char in dangerous_chars):
        raise ValueError("å…³é”®è¯åŒ…å«éæ³•å­—ç¬¦ï¼ˆä¸å…è®¸: ; \\n \\r ç­‰ï¼‰")

    return keywords


def validate_max_results(max_results: int) -> int:
    """éªŒè¯æœ€å¤§ç»“æœæ•°"""
    if max_results < 1:
        raise ValueError("max-results å¿…é¡»å¤§äº 0")
    if max_results > 1000:
        raise ValueError("max-results ä¸èƒ½è¶…è¿‡ 1000ï¼ˆAPI é™åˆ¶ï¼‰")
    return max_results


def validate_collection_key(collection_key: Optional[str]) -> Optional[str]:
    """éªŒè¯é›†åˆ KEY"""
    if collection_key is None:
        return None

    collection_key = collection_key.strip()
    if not collection_key:
        raise ValueError("é›†åˆ KEY ä¸èƒ½ä¸ºç©ºå­—ç¬¦ä¸²")

    # Zotero collection keys é€šå¸¸æ˜¯ uppercase alphanumeric
    if not collection_key.replace("_", "").replace("-", "").isalnum():
        print(f"âš ï¸  è­¦å‘Š: é›†åˆ KEY '{collection_key}' æ ¼å¼å¯èƒ½ä¸æ­£ç¡®")

    return collection_key


def load_config():
    """åŠ è½½å¹¶éªŒè¯é…ç½®"""
    try:
        config = ConfigLoader.load_zotero_config()
        return (
            config["library_id"],
            config["api_key"],
            config["collection_key"],
            config["enable_chinaxiv"],
        )
    except ConfigError as e:
        print(f"\nâŒ é…ç½®é”™è¯¯: {e}")
        print("\nğŸ’¡ å¿«é€Ÿé…ç½®:")
        print("   1. å¤åˆ¶ .env.example åˆ° .env:")
        print("      cp .env.example .env")
        print("   2. åœ¨ .env ä¸­å¡«å…¥ä½ çš„ Zotero å‡­è¯:")
        print("      ZOTERO_LIBRARY_ID=your_library_id")
        print("      ZOTERO_API_KEY=your_api_key")
        print("      TEMP_COLLECTION_KEY=your_collection_key")
        print("   3. é‡æ–°è¿è¡Œç¨‹åº\n")
        sys.exit(1)


def _estimate_cache_hit_rate(enable_openalex_ranking: bool) -> float:
    """
    ä¼°ç®— OpenAlex ç¼“å­˜å‘½ä¸­ç‡

    Args:
        enable_openalex_ranking: æ˜¯å¦å¯ç”¨ OpenAlex æ’åº

    Returns:
        ä¼°ç®—çš„ç¼“å­˜å‘½ä¸­ç‡ (0.0-1.0)
    """
    from paperflow.clients.openalex_client import OpenAlexClient

    if not enable_openalex_ranking:
        return 0.0

    try:
        client = OpenAlexClient()
        cache_file = client.cache_file

        # æ£€æŸ¥ç¼“å­˜æ–‡ä»¶æ˜¯å¦å­˜åœ¨å’Œå¤§å°
        if not cache_file.exists():
            return 0.0  # æ— ç¼“å­˜ï¼Œé¦–æ¬¡è¿è¡Œ

        # æ£€æŸ¥ç¼“å­˜æ–‡ä»¶å¤§å°ï¼ˆä¼°ç®—å‘½ä¸­ç‡ï¼‰
        size_mb = cache_file.stat().st_size / (1024 * 1024)

        # åŸºäºç¼“å­˜æ–‡ä»¶å¤§å°çš„ç»éªŒä¼°ç®—
        # < 0.1 MB: çº¦ 10% å‘½ä¸­ç‡ï¼ˆæ–°ç¼“å­˜ï¼‰
        # 0.1-1 MB: çº¦ 50% å‘½ä¸­ç‡
        # > 1 MB: çº¦ 80% å‘½ä¸­ç‡ï¼ˆæˆç†Ÿç¼“å­˜ï¼‰

        if size_mb < 0.1:
            return 0.1
        elif size_mb < 1.0:
            return 0.5
        else:
            return 0.8

    except Exception:
        return 0.0  # ä¿å®ˆä¼°ç®—ï¼šæ— ç¼“å­˜


async def search_papers(
    keywords: str,
    max_results: int = 20,
    download_pdfs: bool = True,
    collection_key: Optional[str] = None,
    enable_chinaxiv: bool = False,
    chinaxiv_keywords: Optional[str] = None,
    enable_openalex_ranking: bool = False,
    openalex_weights: Optional[dict] = None,
    target_results: Optional[int] = None,
    collection_only_dupcheck: bool = False,
    auto_preload: bool = True,
):
    """
    æœç´¢å¹¶ä¿å­˜è®ºæ–‡åˆ°æŒ‡å®šé›†åˆ

    Args:
        keywords: æœç´¢å…³é”®è¯
        max_results: æœ€å¤§ç»“æœæ•°ï¼ˆé»˜è®¤ 20ï¼‰
        download_pdfs: æ˜¯å¦ä¸‹è½½ PDF
        collection_key: ç›®æ ‡é›†åˆ KEYï¼ˆé»˜è®¤ temp é›†åˆï¼‰
        enable_chinaxiv: æ˜¯å¦å¯ç”¨ ChinaXiv æ¥æº
        chinaxiv_keywords: ChinaXiv ä¸­æ–‡å…³é”®è¯ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨ keywordsï¼‰
        enable_openalex_ranking: æ˜¯å¦å¯ç”¨ OpenAlex æœŸåˆŠæŒ‡æ ‡æ’åº
        openalex_weights: OpenAlex æŒ‡æ ‡æƒé‡é…ç½®
        target_results: ç›®æ ‡ä¿å­˜æ•°é‡ï¼ˆè‡ªåŠ¨è¡¥å……åˆ°è¯¥æ•°é‡ï¼‰
        collection_only_dupcheck: æ˜¯å¦ä»…åœ¨ç›®æ ‡é›†åˆå†…æŸ¥é‡
        auto_preload: æ˜¯å¦è‡ªåŠ¨é¢„çƒ­ç¼“å­˜ï¼ˆé»˜è®¤ Trueï¼‰
    """
    # Load configuration
    ZOTERO_LIBRARY_ID, ZOTERO_API_KEY, _, _ = load_config()

    print("\n" + "=" * 70)
    print("è®ºæ–‡çµæ´»æœç´¢å·¥å…· | Flexible Search")
    print("=" * 70)
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    if enable_chinaxiv:
        # åŒè¯­æ¨¡å¼
        chinaxiv_kw = chinaxiv_keywords if chinaxiv_keywords else keywords
        print(f"ğŸŒ åŒè¯­æ¨¡å¼: åœ¨ arXiv ä¸­ä½¿ç”¨ä¸­è‹±å…³é”®è¯æœç´¢")
        print(f"  ğŸ“ è‹±æ–‡å…³é”®è¯: {keywords}")
        print(f"  ğŸ“ ä¸­æ–‡å…³é”®è¯: {chinaxiv_kw}")
        print(f"  ğŸ“Š æ¯ç§è¯­è¨€ä¸Šé™: 30 ç¯‡ï¼Œæ€»ä¸Šé™: 60 ç¯‡")
        print(f"  ğŸ” æ•°æ®æ¥æº: arXivï¼ˆå»é‡åˆå¹¶ï¼‰")
    else:
        # å•è¯­æ¨¡å¼
        print(f"æœç´¢å…³é”®è¯: {keywords}")
        print(f"æœ€å¤§ç»“æœæ•°: {max_results}")
        print(f"æ•°æ®æ¥æº: arXiv")

    if target_results:
        print(f"ç›®æ ‡ä¿å­˜æ•°é‡: {target_results}ï¼ˆè‡ªåŠ¨è¡¥å……ï¼‰")
    print(f"ç›®æ ‡é›†åˆ: {collection_key} (temp)")
    print(f"OpenAlex æ’åº: {'å¯ç”¨' if enable_openalex_ranking else 'ç¦ç”¨'}")
    if enable_openalex_ranking and openalex_weights:
        print(f"  æƒé‡é…ç½®: {openalex_weights}")
    print(f"æŸ¥é‡æ¨¡å¼: {'é›†åˆå†…ï¼ˆæ›´å¿«ï¼‰' if collection_only_dupcheck else 'å…¨å±€ï¼ˆæ›´å®‰å…¨ï¼‰'}")
    print(f"ä¸‹è½½ PDF: {'æ˜¯' if download_pdfs else 'å¦'}")
    print("=" * 70 + "\n")

    try:
        # Initialize collector with ChinaXiv and OpenAlex support
        # åˆå§‹åŒ–é‡‡é›†å™¨ï¼ˆæ”¯æŒ ChinaXiv å’Œ OpenAlexï¼‰
        collector = ArxivZoteroCollector(
            zotero_library_id=ZOTERO_LIBRARY_ID,
            zotero_api_key=ZOTERO_API_KEY,
            collection_key=collection_key,
            enable_chinaxiv=enable_chinaxiv,
            enable_openalex_ranking=enable_openalex_ranking,
            openalex_weights=openalex_weights,
            collection_only_dupcheck=collection_only_dupcheck,
        )

        # è‡ªåŠ¨é¢„çƒ­ç¼“å­˜ï¼ˆå¦‚æœå¯ç”¨ OpenAlex ä¸”ç¼“å­˜ä¸ºç©ºï¼‰
        if enable_openalex_ranking and auto_preload:
            from paperflow.clients.openalex_client import OpenAlexClient

            print("ğŸ”„ æ£€æŸ¥ OpenAlex ç¼“å­˜çŠ¶æ€...")
            openalex_client = OpenAlexClient()

            # æ£€æŸ¥æ˜¯å¦éœ€è¦é¢„çƒ­
            should_preload = False
            if not openalex_client.cache_file.exists():
                print("   ğŸ“­ ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨ï¼Œé¦–æ¬¡è¿è¡Œ")
                should_preload = True
            else:
                size_mb = openalex_client.cache_file.stat().st_size / (1024 * 1024)
                if size_mb < 0.01:  # å°äº 10KB è§†ä¸ºç©ºç¼“å­˜
                    print(f"   ğŸ“­ ç¼“å­˜æ–‡ä»¶ä¸ºç©º ({size_mb:.3f} MB)")
                    should_preload = True
                else:
                    print(f"   âœ… ç¼“å­˜å·²å­˜åœ¨ ({size_mb:.2f} MB)")

            # æ‰§è¡Œé¢„çƒ­
            if should_preload:
                print("\nğŸš€ è‡ªåŠ¨é¢„çƒ­å¸¸è§æœŸåˆŠç¼“å­˜ï¼ˆæå‡åç»­æŸ¥è¯¢é€Ÿåº¦ï¼‰...")
                print("   é¢„è®¡è€—æ—¶: 15-30 ç§’ï¼ˆä»…é¦–æ¬¡è¿è¡Œï¼‰\n")

                openalex_client.auto_preload_common_journals(silent=False)

                # æ˜¾ç¤ºç¼“å­˜å¤§å°
                if openalex_client.cache_file.exists():
                    new_size_mb = openalex_client.cache_file.stat().st_size / (1024 * 1024)
                    print(f"\nâœ… ç¼“å­˜é¢„çƒ­å®Œæˆï¼å½“å‰ç¼“å­˜å¤§å°: {new_size_mb:.2f} MB")
                    print("   åç»­æŸ¥è¯¢å°†ä½¿ç”¨ç¼“å­˜ï¼Œé€Ÿåº¦æå‡ 70-90%\n")
                else:
                    print("\nâš ï¸  ç¼“å­˜é¢„çƒ­å¯èƒ½æœªå®Œå…¨æˆåŠŸï¼Œä½†ä¸å½±å“ç»§ç»­ä½¿ç”¨\n")

            # æ¸…ç†å®¢æˆ·ç«¯
            openalex_client.close()

        # è‡ªåŠ¨è¡¥å……é€»è¾‘ï¼ˆæ™ºèƒ½ç­–ç•¥ï¼‰
        if target_results:
            # æ™ºèƒ½è¡¥å……ç­–ç•¥ï¼šåŸºäºç¼“å­˜å‘½ä¸­ç‡åŠ¨æ€è°ƒæ•´åˆå§‹æœç´¢æ•°é‡
            cache_hit_rate = _estimate_cache_hit_rate(enable_openalex_ranking)

            if cache_hit_rate > 0.5:
                # é«˜ç¼“å­˜å‘½ä¸­ç‡ï¼ˆ>50%ï¼‰ï¼šåˆå§‹æœç´¢ 1.2å€
                multiplier = 1.2
                strategy = "é«˜ç¼“å­˜å‘½ä¸­ç‡"
            elif cache_hit_rate > 0.2:
                # ä¸­ç­‰ç¼“å­˜å‘½ä¸­ç‡ï¼ˆ20-50%ï¼‰ï¼šåˆå§‹æœç´¢ 1.5å€
                multiplier = 1.5
                strategy = "ä¸­ç­‰ç¼“å­˜å‘½ä¸­ç‡"
            else:
                # ä½ç¼“å­˜å‘½ä¸­ç‡ï¼ˆ<20%ï¼‰æˆ–é¦–æ¬¡è¿è¡Œï¼šåˆå§‹æœç´¢ 2.0å€
                multiplier = 2.0
                strategy = "ä½ç¼“å­˜å‘½ä¸­ç‡ï¼ˆé¦–æ¬¡è¿è¡Œï¼‰"

            initial_results = int(max_results * multiplier)
            print(f"ğŸ“Š æ™ºèƒ½è¡¥å……æ¨¡å¼ï¼š{strategy}")
            print(f"   åˆå§‹æœç´¢: {initial_results} ç¯‡")
            print(f"   ç›®æ ‡ä¿å­˜: {target_results} ç¯‡")
            print(f"   é¢„ä¼°ç¼“å­˜å‘½ä¸­ç‡: {cache_hit_rate * 100:.0f}%\n")

            # Configure search parameters with initial results
            search_params = ArxivSearchParams(keywords=[keywords], max_results=initial_results)

            print(f"æ­£åœ¨æœç´¢è®ºæ–‡æ¥æº...")
            print(f"æç¤º: æœ¬åœ°æ¨¡å¼ï¼Œä¸å½±å“äº‘ç«¯è‡ªåŠ¨é‡‡é›†\n")

            # Run collection with multi-source support
            # æ‰§è¡Œé‡‡é›†ï¼ˆæ”¯æŒå¤šæ¥æºï¼‰
            successful, failed = await collector.run_manual_collection_async(
                search_params=search_params,
                download_pdfs=download_pdfs,
                use_all_sources=enable_chinaxiv,  # å¯ç”¨å¤šæ¥æºæœç´¢
                chinaxiv_keywords=chinaxiv_keywords,  # ChinaXiv ä¸­æ–‡å…³é”®è¯
            )

            # æ£€æŸ¥æ˜¯å¦éœ€è¦è¡¥å……
            if successful < target_results:
                print(f"\nâš ï¸  å½“å‰ä¿å­˜ {successful} ç¯‡ï¼Œç›®æ ‡æ˜¯ {target_results} ç¯‡")
                print(f"æ­£åœ¨è¡¥å……æ›´å¤šè®ºæ–‡...")

                # æ™ºèƒ½è¡¥å……ï¼šåŠ¨æ€è°ƒæ•´è¡¥å……æ•°é‡
                # å¦‚æœç¬¬ä¸€æ¬¡æœç´¢æˆåŠŸç‡å¾ˆä½ï¼Œå¢åŠ è¡¥å……æ•°é‡
                success_rate = successful / initial_results
                if success_rate < 0.3:
                    # æˆåŠŸç‡å¾ˆä½ï¼ˆ<30%ï¼‰ï¼Œå¯èƒ½æ˜¯é‡å¤ç‡é«˜ï¼Œå¤§å¹…å¢åŠ è¡¥å……
                    additional_multiplier = 3
                elif success_rate < 0.6:
                    # æˆåŠŸç‡ä¸­ç­‰ï¼ˆ30-60%ï¼‰ï¼Œé€‚åº¦å¢åŠ è¡¥å……
                    additional_multiplier = 2
                else:
                    # æˆåŠŸç‡è¾ƒé«˜ï¼ˆ>60%ï¼‰ï¼Œå°‘é‡è¡¥å……
                    additional_multiplier = 1.5

                needed = target_results - successful
                additional_results = min(int(needed * additional_multiplier), 100)  # æœ€å¤šå†æœ100ç¯‡

                print(f"è¡¥å……æœç´¢: å†æœç´¢ {additional_results} ç¯‡\n")

                # æ–°çš„æœç´¢å‚æ•°ï¼ˆé¿å…é‡å¤ï¼‰
                search_paramsè¡¥å…… = ArxivSearchParams(
                    keywords=[keywords], max_results=additional_results
                )

                # ç»§ç»­é‡‡é›†
                additional_successful, additional_failed = (
                    await collector.run_manual_collection_async(
                        search_params=search_paramsè¡¥å……,
                        download_pdfs=download_pdfs,
                        use_all_sources=enable_chinaxiv,
                    )
                )

                successful += additional_successful
                failed += additional_failed

                if successful >= target_results:
                    print(f"\nâœ… å·²è¾¾åˆ°ç›®æ ‡æ•°é‡: {successful} ç¯‡")
                else:
                    print(f"\nâš ï¸  å·²å°½åŠ›è¡¥å……ï¼Œå½“å‰: {successful} ç¯‡ï¼ˆå¯èƒ½é‡åˆ°é‡å¤æˆ–APIé™åˆ¶ï¼‰")
            else:
                print(f"\nâœ… å·²è¾¾åˆ°ç›®æ ‡æ•°é‡: {successful} ç¯‡")
        else:
            # é…ç½®æœç´¢å‚æ•°ï¼ˆæ— æ—¥æœŸè¿‡æ»¤ - è·å–æœ€æ–°è®ºæ–‡ï¼‰
            # é…ç½®æœç´¢å‚æ•°ï¼ˆæ— æ—¥æœŸè¿‡æ»¤ - è·å–æœ€æ–°è®ºæ–‡ï¼‰
            search_params = ArxivSearchParams(keywords=[keywords], max_results=max_results)

            print(f"æ­£åœ¨æœç´¢è®ºæ–‡æ¥æº...")
            print(f"æç¤º: æœ¬åœ°æ¨¡å¼ï¼Œä¸å½±å“äº‘ç«¯è‡ªåŠ¨é‡‡é›†\n")

            # Run collection with multi-source support
            # æ‰§è¡Œé‡‡é›†ï¼ˆæ”¯æŒå¤šæ¥æºï¼‰
            successful, failed = await collector.run_manual_collection_async(
                search_params=search_params,
                download_pdfs=download_pdfs,
                use_all_sources=enable_chinaxiv,  # å¯ç”¨å¤šæ¥æºæœç´¢
                chinaxiv_keywords=chinaxiv_keywords,  # ChinaXiv ä¸­æ–‡å…³é”®è¯
            )

        print(f"\n{'=' * 70}")
        print("æœç´¢å®Œæˆ | Search Complete")
        print(f"{'=' * 70}")
        print(f"ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"\næ€»è®¡:")
        print(f"  æˆåŠŸé‡‡é›†: {successful} ç¯‡")
        print(f"  å¤±è´¥: {failed} ç¯‡")
        print(f"  ä¿å­˜ä½ç½®: Temp é›†åˆ ({collection_key})")
        print("\næç¤º: é‡å¤æ£€æµ‹å·²å¯ç”¨ï¼Œå·²å­˜åœ¨çš„è®ºæ–‡ä¼šè¢«è·³è¿‡")
        print("=" * 70 + "\n")

        return successful, failed

    except Exception as e:
        print(f"\nâŒ é”™è¯¯ | Error: {e}")
        import traceback

        traceback.print_exc()
        return 0, 0


async def main():
    """ä¸»å‡½æ•° - å‘½ä»¤è¡Œæ¥å£"""
    # åŠ è½½é…ç½®ï¼ˆç§»é™¤ç¡¬ç¼–ç å¯†é’¥ï¼‰
    ZOTERO_LIBRARY_ID, ZOTERO_API_KEY, TEMP_COLLECTION_KEY, ENABLE_CHINAXIV = load_config()

    parser = argparse.ArgumentParser(
        description="æœ¬åœ°æ¨¡å¼ï¼šçµæ´»çš„ arXiv è®ºæ–‡æœç´¢å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ | Examples:
  # æœç´¢è‡ªåŠ¨é©¾é©¶ç›¸å…³è®ºæ–‡ï¼ˆé»˜è®¤20ç¯‡ï¼‰
  python search_papers.py --keywords "autonomous driving"

  # æ·±åº¦å­¦ä¹ å’Œè®¡ç®—æœºè§†è§‰
  python search_papers.py --keywords '"deep learning" AND "computer vision"'

  # æŒ‡å®šç»“æœæ•°é‡
  python search_papers.py --keywords "V2X communication" --max-results 50

  # åªæœç´¢å…ƒæ•°æ®ï¼Œä¸ä¸‹è½½ PDF
  python search_papers.py --keywords "reinforcement learning" --no-pdf

  # æ˜¾å¼ä¸‹è½½ PDFï¼ˆé»˜è®¤è¡Œä¸ºï¼Œå¯çœç•¥ --pdf å‚æ•°ï¼‰
  python search_papers.py --keywords "deep learning" --pdf

  # å¯ç”¨ OpenAlex æœŸåˆŠæŒ‡æ ‡æ’åº
  python search_papers.py --keywords "deep learning" --enable-openalex

  # è‡ªå®šä¹‰ OpenAlex æƒé‡
  python search_papers.py --keywords "neural networks" --enable-openalex \\
    --openalex-weights '{"cited_by_percentile": 0.7, "h_index": 0.2, "impact_factor": 0.1}'

  # åŒè¯­æ¨¡å¼ï¼šåœ¨ arXiv ä¸­ä½¿ç”¨ä¸­è‹±å…³é”®è¯æœç´¢
  # è‹±æ–‡å…³é”®è¯æœç´¢ 30 ç¯‡ + ä¸­æ–‡å…³é”®è¯æœç´¢ 30 ç¯‡ï¼Œå»é‡åæ€»ä¸Šé™ 60 ç¯‡
  python search_papers.py --keywords "autonomous driving" --chinaxiv-keywords "è‡ªåŠ¨é©¾é©¶" -x

  # åŒè¯­æ¨¡å¼ï¼šç›¸åŒçš„ä¸­è‹±å…³é”®è¯
  python search_papers.py --keywords "è‡ªåŠ¨é©¾é©¶" --enable-chinaxiv

  # ç›®æ ‡æ•°é‡è‡ªåŠ¨è¡¥å……ï¼ˆåˆå§‹æœç´¢ 75 ç¯‡ï¼Œç¡®ä¿ä¿å­˜ 50 ç¯‡ï¼‰
  python search_papers.py --keywords "deep learning" --max-results 50 --target-results 50

  # é›†åˆå†…æŸ¥é‡ï¼ˆæ›´å¿«ï¼Œé€‚åˆå•ä¸€é›†åˆä½¿ç”¨ï¼‰
  python search_papers.py --keywords "autonomous driving" --collection-only-dupcheck

  # ç¦ç”¨è‡ªåŠ¨ç¼“å­˜é¢„çƒ­ï¼ˆå¦‚æœå·²æœ‰ç¼“å­˜ï¼‰
  python search_papers.py --keywords "deep learning" --enable-openalex --no-auto-preload

æ³¨æ„ | Notes:
  - æœ¬åœ°æ¨¡å¼ï¼Œä¸å½±å“äº‘ç«¯è‡ªåŠ¨é‡‡é›†ï¼ˆscripts/run_auto_collection.pyï¼‰
  - ä¿å­˜åˆ° Temp é›†åˆï¼ˆAQNIN4ZZï¼‰ï¼Œä¸äº‘ç«¯æ¨¡å¼åˆ†å¼€
  - é‡å¤æ£€æµ‹å·²å¯ç”¨ï¼Œè‡ªåŠ¨è·³è¿‡å·²å­˜åœ¨çš„è®ºæ–‡
  - åŒè¯­æ¨¡å¼ï¼ˆ--enable-chinaxivï¼‰ï¼šåœ¨ arXiv ä¸­ä½¿ç”¨ä¸­è‹±å…³é”®è¯åˆ†åˆ«æœç´¢ï¼Œæ€»ä¸Šé™60ç¯‡ï¼Œè‡ªåŠ¨å»é‡
  - ä½¿ç”¨ --chinaxiv-keywords æŒ‡å®šä¸­æ–‡å…³é”®è¯ï¼ˆä¸æŒ‡å®šåˆ™ä½¿ç”¨ç›¸åŒå…³é”®è¯ï¼‰
  - PDF ä¸‹è½½ï¼šé»˜è®¤å¯ç”¨ï¼ˆ--pdfï¼‰ï¼Œä½¿ç”¨ --no-pdf ä»…ä¿å­˜å…ƒæ•°æ®
  - OpenAlex æ’åºæŒ‰æœŸåˆŠå½±å“åŠ›æŒ‡æ ‡ç»¼åˆè¯„åˆ†ï¼Œä¼˜å…ˆæ˜¾ç¤ºé«˜è´¨é‡è®ºæ–‡
  - è‡ªåŠ¨é¢„çƒ­ï¼šå¯ç”¨ OpenAlex æ—¶é¦–æ¬¡è¿è¡Œä¼šè‡ªåŠ¨é¢„åŠ è½½å¸¸è§æœŸåˆŠç¼“å­˜ï¼ˆ15-30ç§’ï¼‰
  - å¦‚éœ€ç¦ç”¨è‡ªåŠ¨é¢„çƒ­ï¼Œä½¿ç”¨ --no-auto-preload å‚æ•°
        """,
    )

    parser.add_argument(
        "--keywords", "-k", type=str, help='æœç´¢å…³é”®è¯ï¼ˆä¾‹å¦‚: "autonomous driving"ï¼‰'
    )

    parser.add_argument(
        "--chinaxiv-keywords",
        "-z",
        type=str,
        help="ä¸­æ–‡å…³é”®è¯ï¼ˆåŒè¯­æ¨¡å¼ï¼šåœ¨ arXiv ä¸­ä½¿ç”¨ä¸­è‹±å…³é”®è¯åˆ†åˆ«æœç´¢ï¼‰",
    )

    parser.add_argument(
        "--max-results",
        "-m",
        type=int,
        default=50,
        metavar="N",
        help="æœ€å¤§ç»“æœæ•°ï¼ˆé»˜è®¤: 50ï¼Œæ‰‹åŠ¨æ¨¡å¼ï¼‰",
    )

    parser.add_argument(
        "--pdf",
        "-p",
        dest="download_pdf",
        action="store_true",
        help="ä¸‹è½½ PDF æ–‡ä»¶ï¼ˆé»˜è®¤å¯ç”¨ï¼‰",
    )

    parser.add_argument(
        "--no-pdf",
        "-n",
        dest="download_pdf",
        action="store_false",
        help="ä¸ä¸‹è½½ PDF æ–‡ä»¶ï¼ˆä»…ä¿å­˜å…ƒæ•°æ®ï¼‰",
    )

    # è®¾ç½®é»˜è®¤å€¼ä¸º Trueï¼ˆä¸‹è½½PDFï¼‰
    parser.set_defaults(download_pdf=True)

    parser.add_argument(
        "--collection",
        "-c",
        type=str,
        default=None,
        help="ç›®æ ‡é›†åˆ KEYï¼ˆé»˜è®¤: TEMP_COLLECTION_KEY ç¯å¢ƒå˜é‡ï¼‰",
    )

    parser.add_argument(
        "--enable-chinaxiv",
        "-x",
        action="store_true",
        help="å¯ç”¨åŒè¯­æ¨¡å¼ï¼ˆä½¿ç”¨ä¸­è‹±å…³é”®è¯åœ¨ arXiv ä¸­åˆ†åˆ«æœç´¢ï¼Œæ€»ä¸Šé™60ç¯‡ï¼‰",
    )

    parser.add_argument(
        "--enable-openalex",
        "-e",
        action="store_true",
        help="å¯ç”¨ OpenAlex æœŸåˆŠæŒ‡æ ‡æ’åºï¼ˆæ‰‹åŠ¨æ¨¡å¼æ¨èï¼Œé»˜è®¤å¯ç”¨ï¼‰",
    )

    parser.add_argument(
        "--no-openalex",
        action="store_true",
        help="ç¦ç”¨ OpenAlex æ’åºï¼ˆè¦†ç›–é»˜è®¤å¯ç”¨ï¼‰",
    )

    parser.add_argument(
        "--openalex-weights",
        "-w",
        type=str,
        help='OpenAlex æŒ‡æ ‡æƒé‡é…ç½®ï¼ˆJSON æ ¼å¼ï¼Œä¾‹å¦‚: \'{"cited_by_percentile": 0.5, "h_index": 0.3, "impact_factor": 0.2}\'ï¼‰',
    )

    parser.add_argument(
        "--target-results",
        "-t",
        type=int,
        metavar="N",
        help="ç›®æ ‡ä¿å­˜æ•°é‡ï¼ˆè‡ªåŠ¨è¡¥å……åˆ°è¯¥æ•°é‡ï¼Œä¾‹å¦‚: --target-results 50ï¼‰",
    )

    parser.add_argument(
        "--collection-only-dupcheck",
        "-d",
        action="store_true",
        help="ä»…åœ¨è¯¥é›†åˆå†…æŸ¥é‡ï¼ˆæ›´å¿«ï¼Œä½†å…è®¸è·¨é›†åˆé‡å¤ï¼‰",
    )

    parser.add_argument(
        "--no-auto-preload",
        action="store_true",
        help="ç¦ç”¨è‡ªåŠ¨ç¼“å­˜é¢„çƒ­ï¼ˆé»˜è®¤ï¼šå¯ç”¨ OpenAlex æ—¶è‡ªåŠ¨é¢„çƒ­ï¼‰",
    )

    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="é¢„è§ˆæ¨¡å¼ï¼šæ˜¾ç¤ºå°†è¦æ‰§è¡Œçš„æ“ä½œä½†ä¸å®é™…æ‰§è¡Œ",
    )

    args = parser.parse_args()

    # éªŒè¯å‚æ•°
    if not args.keywords:
        parser.error(
            'å¿…é¡»æä¾› --keywords å‚æ•°\nç¤ºä¾‹: python search_papers.py --keywords "autonomous driving"'
        )

    # è¾“å…¥éªŒè¯
    try:
        args.keywords = validate_keywords(args.keywords)
        args.max_results = validate_max_results(args.max_results)
        args.collection = validate_collection_key(args.collection)
    except ValueError as e:
        parser.error(f"å‚æ•°éªŒè¯å¤±è´¥: {e}")

    # ä½¿ç”¨é»˜è®¤ collection_key å¦‚æœæœªæŒ‡å®š
    if args.collection is None:
        args.collection = TEMP_COLLECTION_KEY

    # æ‰‹åŠ¨æ¨¡å¼é»˜è®¤å¯ç”¨ OpenAlex æ’åºï¼ˆé™¤éç”¨æˆ·æ˜ç¡®ç¦ç”¨ï¼‰
    if not args.no_openalex and not args.enable_openalex:
        args.enable_openalex = True

    # è§£æ OpenAlex æƒé‡é…ç½®
    openalex_weights = None
    if args.openalex_weights:
        try:
            import json

            openalex_weights = json.loads(args.openalex_weights)
            # éªŒè¯æƒé‡æ€»å’Œ
            total_weight = sum(openalex_weights.values())
            if abs(total_weight - 1.0) > 0.01:
                print(f"è­¦å‘Š: æƒé‡æ€»å’Œä¸º {total_weight}ï¼Œå°†è‡ªåŠ¨å½’ä¸€åŒ–")
        except json.JSONDecodeError:
            parser.error("--openalex-weights å¿…é¡»æ˜¯æœ‰æ•ˆçš„ JSON æ ¼å¼")
        except Exception as e:
            parser.error(f"è§£ææƒé‡é…ç½®å¤±è´¥: {e}")

    # Dry-run æ¨¡å¼ï¼šæ˜¾ç¤ºé…ç½®ä½†ä¸æ‰§è¡Œ
    if args.dry_run:
        if RICH_AVAILABLE:
            console = Console()
            console.print("\n[bold cyan]ğŸ” Dry-Run é¢„è§ˆæ¨¡å¼[/bold cyan]\n")

            table = Table(show_header=True, header_style="bold magenta")
            table.add_column("é…ç½®é¡¹", style="cyan", width=30)
            table.add_column("å€¼", style="yellow")

            table.add_row("æœç´¢å…³é”®è¯", args.keywords)
            table.add_row("æœ€å¤§ç»“æœæ•°", str(args.max_results))
            table.add_row("ä¸‹è½½ PDF", "æ˜¯" if args.download_pdf else "å¦")
            table.add_row("ç›®æ ‡é›†åˆ", args.collection)
            table.add_row(
                "å¯ç”¨ ChinaXiv", "æ˜¯" if args.enable_chinaxiv or ENABLE_CHINAXIV else "å¦"
            )
            table.add_row("å¯ç”¨ OpenAlex", "æ˜¯" if args.enable_openalex else "å¦")
            if args.enable_openalex and openalex_weights:
                table.add_row("OpenAlex æƒé‡", str(openalex_weights))
            if args.target_results:
                table.add_row("ç›®æ ‡ä¿å­˜æ•°é‡", str(args.target_results))
            table.add_row("é›†åˆå†…æŸ¥é‡", "æ˜¯" if args.collection_only_dupcheck else "å¦")
            table.add_row(
                "è‡ªåŠ¨é¢„çƒ­ç¼“å­˜", "å¦" if args.no_auto_preload else "æ˜¯ï¼ˆå¦‚æœå¯ç”¨ OpenAlexï¼‰"
            )

            console.print(table)
            console.print(
                "\n[dim]ğŸ’¡ è¿™æ˜¯é¢„è§ˆæ¨¡å¼ï¼Œä¸ä¼šå®é™…æ‰§è¡Œæ“ä½œã€‚å»æ‰ --dry-run å‚æ•°ä»¥è¿è¡Œç¨‹åºã€‚[/dim]\n"
            )
        else:
            print("\nğŸ” Dry-Run é¢„è§ˆæ¨¡å¼\n")
            print(f"æœç´¢å…³é”®è¯: {args.keywords}")
            print(f"æœ€å¤§ç»“æœæ•°: {args.max_results}")
            print(f"ä¸‹è½½ PDF: {'æ˜¯' if args.download_pdf else 'å¦'}")
            print(f"ç›®æ ‡é›†åˆ: {args.collection}")
            print(f"å¯ç”¨ ChinaXiv: {'æ˜¯' if args.enable_chinaxiv or ENABLE_CHINAXIV else 'å¦'}")
            print(f"å¯ç”¨ OpenAlex: {'æ˜¯' if args.enable_openalex else 'å¦'}")
            if args.enable_openalex and openalex_weights:
                print(f"OpenAlex æƒé‡: {openalex_weights}")
            if args.target_results:
                print(f"ç›®æ ‡ä¿å­˜æ•°é‡: {args.target_results}")
            print(f"é›†åˆå†…æŸ¥é‡: {'æ˜¯' if args.collection_only_dupcheck else 'å¦'}")
            print(f"è‡ªåŠ¨é¢„çƒ­ç¼“å­˜: {'å¦' if args.no_auto_preload else 'æ˜¯ï¼ˆå¦‚æœå¯ç”¨ OpenAlexï¼‰'}")
            print("\nğŸ’¡ è¿™æ˜¯é¢„è§ˆæ¨¡å¼ï¼Œä¸ä¼šå®é™…æ‰§è¡Œæ“ä½œã€‚å»æ‰ --dry-run å‚æ•°ä»¥è¿è¡Œç¨‹åºã€‚\n")

        sys.exit(0)

    # è¿è¡Œæœç´¢
    collector_instance = None
    try:
        # Initialize logger
        collector_instance = ArxivZoteroCollector(
            zotero_library_id=ZOTERO_LIBRARY_ID,
            zotero_api_key=ZOTERO_API_KEY,
            collection_key=CollectionLogger.LOG_COLLECTION_KEY,  # Use log collection
        )
        logger = CollectionLogger(collector_instance.zotero_client)
        logger.start_timer()

        successful, failed = await search_papers(
            keywords=args.keywords,
            max_results=args.max_results,
            download_pdfs=args.download_pdf,
            collection_key=args.collection,
            enable_chinaxiv=args.enable_chinaxiv or ENABLE_CHINAXIV,
            chinaxiv_keywords=args.chinaxiv_keywords,
            enable_openalex_ranking=args.enable_openalex,
            openalex_weights=openalex_weights,
            target_results=args.target_results,
            collection_only_dupcheck=args.collection_only_dupcheck,
            auto_preload=not args.no_auto_preload,
        )

        # Generate and upload log
        print("\nç”Ÿæˆæ—¥å¿—æ–‡ä»¶...")
        source_stats = {
            "arxiv": {
                "found": successful + failed,
                "successful": successful,
                "duplicates": 0,
                "failed": failed,
            },
            "chinaxiv": {
                "found": 0,
                "successful": 0,
                "duplicates": 0,
                "failed": 0,
            },  # TODO: Track separately
        }
        log_content = logger.generate_manual_log(
            keywords=args.keywords,
            max_results=args.max_results,
            download_pdfs=args.download_pdf,
            openalex_enabled=args.enable_openalex,
            openalex_stats=None,  # TODO: Collect stats
            source_stats=source_stats,
        )
        log_filename = logger.generate_filename(mode="manual")

        if await logger.upload_to_zotero(log_content, log_filename):
            print(f"âœ“ æ—¥å¿—å·²ä¸Šä¼ åˆ° Zotero: {log_filename}")
        else:
            print(f"âœ— æ—¥å¿—ä¸Šä¼ å¤±è´¥: {log_filename}")

        # æ ¹æ®ç»“æœè®¾ç½®é€€å‡ºç 
        if failed > 0:
            sys.exit(1)  # æœ‰å¤±è´¥çš„æƒ…å†µ
        else:
            sys.exit(0)  # å…¨éƒ¨æˆåŠŸ

    except KeyboardInterrupt:
        print("\n\næ“ä½œå·²å–æ¶ˆ | Operation cancelled")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ æœªé¢„æœŸçš„é”™è¯¯ | Unexpected error: {e}")
        sys.exit(1)
    finally:
        if collector_instance:
            await collector_instance.close()


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except RuntimeError as e:
        if "asyncio.run() cannot be called from a running event loop" in str(e):
            # Handle cases where event loop is already running (e.g., in IDE)
            loop = asyncio.get_event_loop()
            loop.run_until_complete(main())
        else:
            raise
